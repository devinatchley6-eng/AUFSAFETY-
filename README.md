# AUFSAFETY

Repository for AUF (Alignment–Utility–Fidelity) research artifacts.

## Current status

- **AUF v0.1 is now treated as an exploratory draft, not a deployment-ready safety standard.**
- New empirical reporting and validation documents have been added to capture critical gaps, failed assumptions, and a revised research roadmap.

## Documents

- `README.MD` — long-form original technical manuscript.
- `docs/AUF_END_TO_END_SPEC.md` — archived closed-form AUF v0.1 architecture (historical reference).
- `docs/EMPIRICAL_AI_SAFETY_RESEARCH_2026.md` — 2026 annual empirical report with deception, scaling, policy, and governance findings.
- `docs/AUF_FRAMEWORK_GAP_ANALYSIS_2025.md` — validation report detailing mathematical, empirical, and implementation failures in AUF v0.1.
- `Code repo` / `Structure` — proposed repository layout references.

## Revised direction (validation-first)

1. Define measurable and operational agency metrics before hard constraints.
2. Develop toy systems and phased empirical protocols before full formalization.
3. Build data-driven and computationally tractable safety controls.
4. Require independent validation prior to any certification claims.
